{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e0a53-8231-409a-9325-0d07e8db4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:00:09,303 - INFO - Using device: cpu\n",
      "2025-10-08 12:00:09,303 - INFO - Loading data from 'delta_learning_dataset.xyz'...\n",
      "2025-10-08 12:00:09,329 - INFO - Successfully loaded 101 configurations.\n",
      "2025-10-08 12:00:09,329 - INFO - Loading base MACE model from MACE-MP_small.model\n",
      "/home/krystynasyty/myenv/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:00:10,364 - INFO - Using r_max=6.00 and atomic numbers from the base model.\n",
      "2025-10-08 12:00:10,365 - INFO - Training set size: 91\n",
      "2025-10-08 12:00:10,366 - INFO - Validation set size: 10\n",
      "2025-10-08 12:00:10,368 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zamrażanie parametrów całego modelu bazowego MACE...\n",
      "Wykryto 128 cech wejściowych do głowy (readout).\n",
      "Wagi nowej głowy 'delta_readout' zostały zainicjalizowane zerami.\n",
      "Hak poprawnie zarejestrowany na ostatnim bloku 'product'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:00:46,325 - INFO - Epoch 1/100 | Train Loss: 4135116184.329174 | Val Loss: 4132480272.240263 | Val MAE (delta): 32125.273065 | Val RMSE (delta): 32125.273066\n",
      "2025-10-08 12:00:46,354 - INFO - New best model saved to delta_model_checkpoints/best_model.pt\n",
      "2025-10-08 12:01:22,925 - INFO - Epoch 2/100 | Train Loss: 4130905435.096497 | Val Loss: 4128285882.890433 | Val MAE (delta): 32092.641073 | Val RMSE (delta): 32092.641073\n",
      "2025-10-08 12:01:22,960 - INFO - New best model saved to delta_model_checkpoints/best_model.pt\n",
      "2025-10-08 12:02:00,428 - INFO - Epoch 3/100 | Train Loss: 4126765102.399417 | Val Loss: 4124105344.384257 | Val MAE (delta): 32060.100339 | Val RMSE (delta): 32060.100340\n",
      "2025-10-08 12:02:00,470 - INFO - New best model saved to delta_model_checkpoints/best_model.pt\n",
      "2025-10-08 12:02:40,681 - INFO - Epoch 4/100 | Train Loss: 4122634790.399866 | Val Loss: 4119927499.410063 | Val MAE (delta): 32027.564090 | Val RMSE (delta): 32027.564091\n",
      "2025-10-08 12:02:40,733 - INFO - New best model saved to delta_model_checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import ase.io\n",
    "from ase.neighborlist import neighbor_list\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import radius_graph\n",
    "\n",
    "# Assuming 'models.py' is in the same directory or accessible in the python path\n",
    "from models import DualReadoutMACE\n",
    "from train import evaluate, DeltaEnergyLoss, load_data, pyg_collate, AtomsDataset\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    # Define your training parameters here instead of using command-line arguments.\n",
    "    \n",
    "    # Path to the dataset file\n",
    "    dataset_path = \"delta_learning_dataset.xyz\"\n",
    "    \n",
    "    # IMPORTANT: Path to the pre-trained base MACE model (.pt or .model file)\n",
    "    # You MUST provide a valid path to your model file.\n",
    "    base_model_path = \"MACE-MP_small.model\" \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs = 100\n",
    "    lr = 1e-3\n",
    "    batch_size = 10\n",
    "    \n",
    "    # Dataset split\n",
    "    validation_split = 0.1\n",
    "    \n",
    "    # Output directory for saved models\n",
    "    output_dir = \"delta_model_checkpoints\"\n",
    "    # --- End of Configuration ---\n",
    "\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    full_atoms_list = load_data(dataset_path)\n",
    "    \n",
    "    # 2. Load base model to get r_max and atomic_numbers for data preparation\n",
    "    logging.info(f\"Loading base MACE model from {base_model_path}\")\n",
    "    try:\n",
    "        base_mace_model = torch.load(base_model_path, map_location=device, weights_only=False)\n",
    "        base_mace_model.to(dtype=torch.float64) # Ensure model is float64\n",
    "        base_mace_model.eval()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Base model file not found at '{base_model_path}'. Please provide a valid path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading base model: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        r_max = base_mace_model.r_max.item()\n",
    "        z_map = {z.item(): i for i, z in enumerate(base_mace_model.atomic_numbers)}\n",
    "        logging.info(f\"Using r_max={r_max:.2f} and atomic numbers from the base model.\")\n",
    "    except AttributeError:\n",
    "        logging.error(\"Could not find 'r_max' or 'atomic_numbers' in the base model. Please check the model file.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting model parameters: {e}\")\n",
    "        return\n",
    "\n",
    "    dataset = AtomsDataset(full_atoms_list, r_max=r_max, z_map=z_map)\n",
    "\n",
    "    # 3. Split data\n",
    "    n_total = len(dataset)\n",
    "    n_val = int(n_total * validation_split)\n",
    "    n_train = n_total - n_val\n",
    "    train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "    \n",
    "    logging.info(f\"Training set size: {len(train_dataset)}\")\n",
    "    logging.info(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = PyGDataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pyg_collate)\n",
    "    val_loader = PyGDataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pyg_collate)\n",
    "    \n",
    "    # 4. Initialize the delta-learning model\n",
    "    model = DualReadoutMACE(base_mace_model)\n",
    "    model.to(dtype=torch.float64) # Ensure the wrapper and new layer are also float64\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Setup optimizer, scheduler, and loss\n",
    "    # The optimizer will only act on parameters where requires_grad is True,\n",
    "    # which is only our new delta_readout layer.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for p in model.parameters() if p.requires_grad], \n",
    "        lr=lr\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "    loss_fn = DeltaEnergyLoss()\n",
    "\n",
    "    # 6. Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    logging.info(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data_dict = batch.to_dict()\n",
    "            if 'pos' in data_dict:\n",
    "                data_dict['positions'] = data_dict.pop('pos')\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data_dict)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        avg_val_loss, mae, rmse = evaluate(model, val_loader, loss_fn, device)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.6f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.6f} | \"\n",
    "            f\"Val MAE (delta): {mae:.6f} | \"\n",
    "            f\"Val RMSE (delta): {rmse:.6f}\"\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "            # We save the state_dict of the entire model, but only the delta_readout part is trained.\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logging.info(f\"New best model saved to {checkpoint_path}\")\n",
    "\n",
    "    logging.info(\"Training complete.\")\n",
    "    final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    logging.info(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4eab6b-9189-47ac-9da7-908f8604cb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
